{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6iGiUbhbLy_",
        "outputId": "89bcf6c1-c917-43a3-e8f0-50d8f34efc6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 3.4071 - accuracy: 0.2641\n",
            "Epoch 2/10\n",
            "1563/1563 [==============================] - 32s 21ms/step - loss: 1.8054 - accuracy: 0.3789\n",
            "Epoch 3/10\n",
            "1563/1563 [==============================] - 32s 20ms/step - loss: 1.7006 - accuracy: 0.4135\n",
            "Epoch 4/10\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 1.6436 - accuracy: 0.4347\n",
            "Epoch 5/10\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 1.5942 - accuracy: 0.4533\n",
            "Epoch 6/10\n",
            "1563/1563 [==============================] - 34s 22ms/step - loss: 1.5697 - accuracy: 0.4577\n",
            "Epoch 7/10\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 1.5837 - accuracy: 0.4518\n",
            "Epoch 8/10\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 1.5564 - accuracy: 0.4606\n",
            "Epoch 9/10\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 1.5122 - accuracy: 0.4780\n",
            "Epoch 10/10\n",
            "1563/1563 [==============================] - 33s 21ms/step - loss: 1.4800 - accuracy: 0.4878\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "Original sample: [[[ 59  62  63]\n",
            "  [ 43  46  45]\n",
            "  [ 50  48  43]\n",
            "  ...\n",
            "  [158 132 108]\n",
            "  [152 125 102]\n",
            "  [148 124 103]]\n",
            "\n",
            " [[ 16  20  20]\n",
            "  [  0   0   0]\n",
            "  [ 18   8   0]\n",
            "  ...\n",
            "  [123  88  55]\n",
            "  [119  83  50]\n",
            "  [122  87  57]]\n",
            "\n",
            " [[ 25  24  21]\n",
            "  [ 16   7   0]\n",
            "  [ 49  27   8]\n",
            "  ...\n",
            "  [118  84  50]\n",
            "  [120  84  50]\n",
            "  [109  73  42]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[208 170  96]\n",
            "  [201 153  34]\n",
            "  [198 161  26]\n",
            "  ...\n",
            "  [160 133  70]\n",
            "  [ 56  31   7]\n",
            "  [ 53  34  20]]\n",
            "\n",
            " [[180 139  96]\n",
            "  [173 123  42]\n",
            "  [186 144  30]\n",
            "  ...\n",
            "  [184 148  94]\n",
            "  [ 97  62  34]\n",
            "  [ 83  53  34]]\n",
            "\n",
            " [[177 144 116]\n",
            "  [168 129  94]\n",
            "  [179 142  87]\n",
            "  ...\n",
            "  [216 184 140]\n",
            "  [151 118  84]\n",
            "  [123  92  72]]]\n",
            "Reconstructed sample: [[0.01002822 0.00634709 0.17864095 0.19838925 0.07004881 0.3053829\n",
            "  0.17829946 0.02811912 0.02134981 0.00339448]]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "# Load the CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "# Convert the labels to one-hot encoded vectors\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "# Define the model architecture\n",
        "input_shape = (32, 32, 3)\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the federated learning system\n",
        "model.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "# Select a sample from the training set to reconstruct\n",
        "sample_idx = 0\n",
        "sample = x_train[sample_idx]\n",
        "\n",
        "# Use the model to reconstruct the sample\n",
        "reconstructed_sample = model.predict(np.expand_dims(sample, axis=0))\n",
        "\n",
        "# Compare the original and reconstructed samples\n",
        "print(f'Original sample: {sample}')\n",
        "print(f'Reconstructed sample: {reconstructed_sample}')\n"
      ]
    }
  ]
}